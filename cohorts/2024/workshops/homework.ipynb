{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open source data ingestion for RAGs with dlt\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=qUNyfR_X2Mo\n",
    "\n",
    "In this hands-on workshop, we’ll learn how to build a data ingestion pipeline using dlt to load data from a REST API into LanceDB so you can have an always up to date RAG.\n",
    "\n",
    "​We’ll cover the following steps:\n",
    "\n",
    "* Extract data from REST APIs\n",
    "* Loading and vectorizing into LanceDB, which unlike other vector DBs stores the data _and_ the embeddings\n",
    "* Incremental loading\n",
    "\n",
    "​By the end of this workshop, you’ll be able to write a portable, OSS data pipeline for your RAG that you can deploy anywhere, such as python notebooks, virtual machines, or orchestrators like Airflow, Dagster or Mage.\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "* Slides: [dlt-LLM-Zoomcamp.pdf](https://github.com/user-attachments/files/16131729/dlt.LLM.Zoomcamp.pdf)\n",
    "* [Google Colab notebook](https://colab.research.google.com/drive/1nNOybHdWQiwUUuJFZu__xvJxL_ADU3xl?usp=sharing) - make a copy to follow along!\n",
    "\n",
    "--- \n",
    "\n",
    "# Homework\n",
    "\n",
    "In the workshop, we extracted contents from two pages in notion titled \"Workshop: Benefits and Perks\" and \"Workshop: Working hours, PTO, and Vacation\". \n",
    "\n",
    "Repeat the same process for a third page titled \"Homework: Employee handbook\" (hidden from public view, but accessible via API key):\n",
    "\n",
    "1. Modify the REST API source to extract only this page.\n",
    "2. Write the output into a separate table called \"homework\".\n",
    "3. Remember to update the table name in all cells where you connect to a lancedb table.\n",
    "\n",
    "To do this you can use the [workshop Colab](https://colab.research.google.com/drive/1nNOybHdWQiwUUuJFZu__xvJxL_ADU3xl?usp=sharing) as a basis.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "No files to update, exiting\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | dlt init rest_api lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\".lancedb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "homework\n",
      "[{'name': 'block_id', 'nullable': False, 'primary_key': True, 'data_type': 'text'}, {'name': 'block_type', 'data_type': 'text', 'nullable': True}, {'name': 'content', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'dedup_sort': 'desc', 'name': 'last_edited_time', 'data_type': 'timestamp', 'nullable': True}, {'name': 'inserted_at_time', 'data_type': 'timestamp', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "UPLOAD\n",
      "Pipeline company_policies load step completed in 5.68 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset notion_pages\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x7f556d408a50> location to store data\n",
      "Load package 1721280192.9822245 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "from dlt.sources.helpers.rest_client.paginators import (\n",
    "    BasePaginator,\n",
    "    JSONResponsePaginator,\n",
    ")\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "from dlt.destinations.adapters import lancedb_adapter\n",
    "\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"homework\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]},\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Notion-Version\": \"2022-06-28\",\n",
    "            },\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"Homework: Employee handbook\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"data_selector\": \"results\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\",\n",
    "                        }\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config, name=\"homework\")\n",
    "\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc),\n",
    "    }\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"homework\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\": {\"dedup_sort\": \"desc\"}},\n",
    ")\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time=dlt.sources.incremental(\n",
    "        \"last_edited_time\",\n",
    "        initial_value=\"2024-06-26T08:16:00.000Z\",\n",
    "        primary_key=(\"block_id\"),\n",
    "    ),\n",
    "):\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not (len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(rest_api_notion_incremental, embed=\"content\"),\n",
    "        table_name=\"homework\",\n",
    "        write_disposition=\"merge\",\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notion_pages____dlt_loads', 'notion_pages____dlt_pipeline_state', 'notion_pages____dlt_version', 'notion_pages___dltSentinelTable', 'notion_pages___homework']\n"
     ]
    }
   ],
   "source": [
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the following questions: \n",
    "\n",
    "## Q1. Rows in LanceDB\n",
    "\n",
    "How many rows does the lancedb table \"notion_pages__homework\" have?\n",
    "\n",
    "* 14\n",
    "* 15\n",
    "* 16\n",
    "* 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "dbtable.to_pandas().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Running the Pipeline: Last edited time\n",
    "\n",
    "In the demo, we created an incremental dlt resource `rest_api_notion_incremental` that keeps track of `last_edited_time`. What value does it store after you've run your pipeline once? (Hint: you will be able to get this value by performing some aggregation function on the column `last_edited_time` of the table)\n",
    "\n",
    "* `Timestamp('2024-07-05 22:34:00+0000', tz='UTC') (OR \"2024-07-05T22:34:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 23:33:00+0000', tz='UTC') (OR \"2024-07-05T23:33:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 23:52:00+0000', tz='UTC') (OR \"2024-07-05T23:52:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 22:56:00+0000', tz='UTC') (OR \"2024-07-05T22:56:00.000Z\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-07-05 23:33:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dbtable.to_pandas()\n",
    "\n",
    "df[\"last_edited_time\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Ask the Assistant \n",
    "\n",
    "Find out with the help of the AI assistant: how many PTO days are the employees entitled to in a year?  \n",
    "\n",
    "* 20\n",
    "* 25\n",
    "* 30\n",
    "* 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Connect to the lancedb table\n",
    "    db = lancedb.connect(\".lancedb\")\n",
    "    dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "    # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # Accept user question\n",
    "        question = input(\"You: \")\n",
    "\n",
    "        # Retrieve the relevant paragraphs on the question\n",
    "        context = retrieve_context_from_lancedb(dbtable, question, top_k=2)\n",
    "\n",
    "        # Create a user prompt using the question and retrieved context\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "        )\n",
    "\n",
    "        # Get the response from the LLM\n",
    "        response = ollama.chat(model=\"llama2-uncensored\", messages=messages)\n",
    "        response_content = response[\"message\"][\"content\"]\n",
    "        print(f\"Assistant: {response_content}\")\n",
    "\n",
    "        # Add the response into the context window\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: According to the handbook, employees are entitled to 30 days of Paid Time Off (PTO) per year. However, if you have not accrued enough PTO, your leave may be unpaid or subject to local law. If you leave our company, we will compensate accrued PTO with your final paycheck according to local law. When the law doesn't have provisions, we will compensate accrued leave to employees who were not terminated for cause. Please contact HR for more information if needed.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/llm-zoomcamp-2024/homework/workshop1\n",
    "* It's possible that your answers won't match exactly. If it's the case, select the closest one.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmzoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
